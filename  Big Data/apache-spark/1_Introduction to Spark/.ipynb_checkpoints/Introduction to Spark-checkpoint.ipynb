{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "## Distributed Computing Before Spark \n",
    "\n",
    "### New Paradigms \n",
    "\n",
    "- Take the computing to the data\n",
    "- Perform local cumpations, aggregate the results *This is the MapReduce paradigm\n",
    "- Don't do expensive transformations\n",
    "- Scale by adding nodes \n",
    "\n",
    "### MapReduce\n",
    "- Framework for processing HDFS data\n",
    "- Original use case was for distributed batch processing\n",
    "- Map-Shuffle-Reduce paradigm for processing \n",
    "- MapReduce daemons package algorithms to where the data is isntead of moving data around \n",
    "- Also manages all of the resources required for the jobs/tasks\n",
    "\n",
    "### How MapReduce Works\n",
    "[<img src=\"https://www.guru99.com/images/Big_Data/061114_0930_Introductio1.png\">](https://www.guru99.com/introduction-to-mapreduce.html)\n",
    "\n",
    "- An input to a MapReduce job is divided into fixed-size pieces called input splits. Input split is a chunk of the input that is consumed by a single map. \n",
    "- Then data in each split is passed to a mapping function to produce output values. \n",
    "- In Shuffling task is to consolidate the relevant records from Mapping phase output. In our example, the same words are clubed together along with their respective frequency.\n",
    "- In Reducing, output values from the Shuffling phase are aggregated. This phase combines values from Shuffling phase and returns a single output value. In short, this phase summarizes the complete dataset.\n",
    "\n",
    "### Challenge with MapReduce \n",
    "- No Real Time Processing\n",
    "- Everything cannot be implemented as MapReduce\n",
    "- Expensive Job creation and startup \n",
    "  - Iterative algorithms means multiple jobs in MapReduce\n",
    "- Skews in the data\n",
    "  - Creates stragglers in Reduce Phase \n",
    "- Suffle is costly operation \n",
    "\n",
    "# Spark \n",
    "- Distributed processing framework\n",
    "- Suports Java, Scala, Python and R \n",
    "  - Easier to use\n",
    "  - Less lines of code (Scala).\n",
    "- Unified stack for different workloads \n",
    "  - SQL/DataFrames, Machine Learning, Graph, Streaming\n",
    "- Supports multiple environments (yarn, docker, kubernetes...).\n",
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/spark-7-1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD \n",
    "\n",
    "RDD = Resilient Distributed Dataset  \n",
    "**Resilient**: Fault-tolerant  \n",
    "**Distributed**: Stored across multiple nodes  \n",
    "**Dataset**: Collection of partitioned data\n",
    "    \n",
    "\n",
    "- **Read only** partitioned collection of records.\n",
    "- Immutable once constructed\n",
    "- Lineage\n",
    "- Rich set of operations\n",
    "- Coarse-Grained Transformation **LOOK AT THIS**\n",
    "  - Coarse  transformation is applied to a set with more than one record\n",
    "  - Grained is a tranformation to a single record\n",
    "  \n",
    "- can only be created through deterministic operations on \n",
    "  - Data \n",
    "  - other RDD's.\n",
    "- Persistence on RDD\n",
    "- Partitioning on RDD\n",
    "- Checkpointing on RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe\n",
    "\n",
    "- A distributed collection of data organized into named columns\n",
    "  - Construct DataFrames via existing dataframe or files in storage system)\n",
    "- Inspired by dataframe in R and Python\n",
    "- Ability to scale from kilobytes in a single computer to petabytes on a large cluster\n",
    "- Support for a wide array of data formats and storage systems \n",
    "- Integration with all big data toolind and infrastructure via Spark\n",
    "- APIs for Python, Java, Scala and R.\n",
    "- Faster than RDD\n",
    " - Spark Python DF = Spark Scala DF < RDD Scala < RDD Python\n",
    " \n",
    "## Datasets \n",
    "\n",
    "- Strongly-typed, immutable collection of objects that are mapped to a relational schema \n",
    "- It is an extension of the Dataframe API\n",
    "- Data is encoded using Spark's Tungsten Binary Format\n",
    "- Uses Catalyst Optimizer\n",
    "\n",
    "- **Best of both words (RDD and DataFrame)**\n",
    "- **Faster than RDD and better memory usage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Architecture\n",
    "\n",
    "- Application: the set of jobs managed by a single driver\n",
    "  - Driver program\n",
    "    - Spark context - the master which communicates with each worker node\n",
    "  - Cluster manager - deals with allocation of resources\n",
    "  - Worker node - node manager \n",
    "    - Executor \n",
    "      - Cache\n",
    "      - Stage - set of tasks that can be executed in parallel\n",
    "        - Job - set of stages executed as a result of an *action* \n",
    "          - Tasks - individual unit of work sent to one executor\n",
    "          \n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\">\n",
    "\n",
    "# Deploy and manage\n",
    "\n",
    "## Deploy \n",
    "\n",
    "- Spark supports multiple environments:\n",
    "  - Deploy on Hadoop, Docker or Kubernetes.\n",
    "## Manage\n",
    "- Hadoop -> Yarn \n",
    "- Docker -> Apache Mesos.\n",
    "\n",
    "## Deploy modes\n",
    "- Client (default) - the driver runs on the machine that the spark application was launched    \n",
    "- Cluster - the driver runs on a random node in a cluster\n",
    "\n",
    "# Spark Development\n",
    "\n",
    "- Spark shell\n",
    "  - Interactive - for learning or data exploration.\n",
    "  - REPL - Read/Evaluate/Print/Loop\n",
    "  - Python or Scala.\n",
    "- Spark applications\n",
    "  - For large scale data processing\n",
    "  - Python, Scala, or Java."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
