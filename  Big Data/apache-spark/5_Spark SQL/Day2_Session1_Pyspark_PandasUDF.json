{"paragraphs":[{"text":"%md\n\n# Pandas UDFs for PySpark\n\n* Pandas UDFs, also called Vectorized UDFs, is a major boost to PySpark performance. Built on top of Apache Arrow, they afford you the best of both worlds—the ability to define low-overhead, high-performance UDFs and write entirely in Python.\n* In Spark 2.3, there are two types of Pandas UDFs: scalar and grouped map. Both are now available in Spark 2.3","user":"anonymous","dateUpdated":"2019-11-13T16:52:09-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Pandas UDFs for PySpark</h1>\n<ul>\n  <li>Pandas UDFs, also called Vectorized UDFs, is a major boost to PySpark performance. Built on top of Apache Arrow, they afford you the best of both worlds—the ability to define low-overhead, high-performance UDFs and write entirely in Python.</li>\n  <li>In Spark 2.3, there are two types of Pandas UDFs: scalar and grouped map. Both are now available in Spark 2.3</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529573_-820857270","id":"20180323-190415_1250638405","dateCreated":"2019-11-13T16:52:09-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52"},{"text":"%md\n* DataFrame.toPandas() in PySpark was painfully inefficient. Basically, it worked by first collecting all rows to the Spark driver. Next, each row would get serialized into Python’s pickle format and sent to a Python worker process. This child process unpickles each row into a huge list of tuples. Finally, a Pandas DataFrame is created from the list using `pandas.DataFrame.from_records()`.\n\n* 2 issues: \n    * even using `CPickle`, Python serialization is a slow process and \n    * creating a pandas.DataFrame using `from_records` must slowly iterate over the list of pure Python data and convert each value to Pandas format. \n\n* Here is where Arrow really shines to help optimize these steps: 1) Once the data is in Arrow memory format, there is no need to serialize/pickle anymore as Arrow data can be sent directly to the Python process, 2) When the Arrow data is received in Python, then pyarrow can utilize zero-copy methods to create a pandas.DataFrame from entire chunks of data at once instead of processing individual scalar values\n\n\nReference: https://arrow.apache.org/blog/2017/07/26/spark-arrow/\n           https://arrow.apache.org/","user":"anonymous","dateUpdated":"2019-11-13T16:52:09-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n  <p>DataFrame.toPandas() in PySpark was painfully inefficient. Basically, it worked by first collecting all rows to the Spark driver. Next, each row would get serialized into Python’s pickle format and sent to a Python worker process. This child process unpickles each row into a huge list of tuples. Finally, a Pandas DataFrame is created from the list using <code>pandas.DataFrame.from_records()</code>.</p></li>\n  <li>\n    <p>2 issues: </p>\n    <ul>\n      <li>even using <code>CPickle</code>, Python serialization is a slow process and</li>\n      <li>creating a pandas.DataFrame using <code>from_records</code> must slowly iterate over the list of pure Python data and convert each value to Pandas format.</li>\n    </ul>\n  </li>\n  <li>\n  <p>Here is where Arrow really shines to help optimize these steps: 1) Once the data is in Arrow memory format, there is no need to serialize/pickle anymore as Arrow data can be sent directly to the Python process, 2) When the Arrow data is received in Python, then pyarrow can utilize zero-copy methods to create a pandas.DataFrame from entire chunks of data at once instead of processing individual scalar values</p></li>\n</ul>\n<p>Reference: <a href=\"https://arrow.apache.org/blog/2017/07/26/spark-arrow/\">https://arrow.apache.org/blog/2017/07/26/spark-arrow/</a><br/> <a href=\"https://arrow.apache.org/\">https://arrow.apache.org/</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529578_139106638","id":"20180323-185215_1234486825","dateCreated":"2019-11-13T16:52:09-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"text":"%md\n```\nfrom pyspark.sql.functions import rand\ndf = spark.range(1 << 22).toDF(\"id\").withColumn(\"x\", rand())\ndf.printSchema()\n%time pdf = df.toPandas()\n```\nOutput: \nCPU times: user 17.4 s, sys: 792 ms, total: 18.1 s\nWall time: 20.7 s","user":"anonymous","dateUpdated":"2019-11-13T16:52:09-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<pre><code>from pyspark.sql.functions import rand\ndf = spark.range(1 &lt;&lt; 22).toDF(&quot;id&quot;).withColumn(&quot;x&quot;, rand())\ndf.printSchema()\n%time pdf = df.toPandas()\n</code></pre>\n<p>Output:<br/>CPU times: user 17.4 s, sys: 792 ms, total: 18.1 s<br/>Wall time: 20.7 s</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529580_1872605499","id":"20180323-184847_21626899","dateCreated":"2019-11-13T16:52:09-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"%md\n```\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n%time pdf = df.toPandas()\n```\n\n\nCPU times: user 40 ms, sys: 32 ms, total: 72 ms                                 \nWall time: 737 ms","user":"anonymous","dateUpdated":"2019-11-13T16:52:09-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<pre><code>spark.conf.set(&quot;spark.sql.execution.arrow.enabled&quot;, &quot;true&quot;)\n%time pdf = df.toPandas()\n</code></pre>\n<p>CPU times: user 40 ms, sys: 32 ms, total: 72 ms<br/>Wall time: 737 ms</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529582_1891890345","id":"20180323-184846_220232457","dateCreated":"2019-11-13T16:52:09-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"text":"%md\n####  pyspark.sql.functions.pandas_udf(f=None, returnType=None, functionType=None)[source]\nCreates a vectorized user defined function (UDF).\n\n##### Parameters:\t\n**f** – user-defined function. A python function if used as a standalone function\n**returnType** – the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.\n**functionType** – an enum value in pyspark.sql.functions.PandasUDFType. Default: SCALAR.\n","user":"anonymous","dateUpdated":"2019-11-13T16:52:09-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>pyspark.sql.functions.pandas_udf(f=None, returnType=None, functionType=None)[source]</h4>\n<p>Creates a vectorized user defined function (UDF).</p>\n<h5>Parameters:</h5>\n<p><strong>f</strong> – user-defined function. A python function if used as a standalone function<br/><strong>returnType</strong> – the return type of the user-defined function. The value can be either a pyspark.sql.types.DataType object or a DDL-formatted type string.<br/><strong>functionType</strong> – an enum value in pyspark.sql.functions.PandasUDFType. Default: SCALAR.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529582_598477474","id":"20180323-155125_1231608171","dateCreated":"2019-11-13T16:52:09-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%md\nSupported SQL Types:\nAll Apache Spark SQL Data Types are supported by Arrow-based conversion except:\n* MapType\n* ArrayType of TimestampType\n* nested StructType\n* Binary Type is supported only when PyArrow is 0.10 or above","user":"anonymous","dateUpdated":"2019-11-13T18:37:35-0600","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Supported SQL Types:<br/>All Apache Spark SQL Data Types are supported by Arrow-based conversion except:<br/>* MapType<br/>* ArrayType of TimestampType<br/>* nested StructType<br/>* Binary Type is supported only when PyArrow is 0.10 or above</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1573691697783_-28279647","id":"20191113-183457_1685816848","dateCreated":"2019-11-13T18:34:57-0600","dateStarted":"2019-11-13T18:37:35-0600","dateFinished":"2019-11-13T18:37:35-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"text":"%md\n### SCALAR\n\n** A scalar UDF defines a transformation** : One or more pandas.Series -> A pandas.Series. The returnType should be a primitive data type, e.g., DoubleType(). The length of the returned pandas.Series must be of the same as the input pandas.Series.\n\nScalar UDFs are used with pyspark.sql.DataFrame.withColumn() and pyspark.sql.DataFrame.select().","user":"anonymous","dateUpdated":"2019-11-13T18:21:09-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>SCALAR</h3>\n<p>** A scalar UDF defines a transformation** : One or more pandas.Series -&gt; A pandas.Series. The returnType should be a primitive data type, e.g., DoubleType(). The length of the returned pandas.Series must be of the same as the input pandas.Series.</p>\n<p>Scalar UDFs are used with pyspark.sql.DataFrame.withColumn() and pyspark.sql.DataFrame.select().</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529583_2140234141","id":"20180323-155210_586426054","dateCreated":"2019-11-13T16:52:09-0600","dateStarted":"2019-11-13T18:21:09-0600","dateFinished":"2019-11-13T18:21:09-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"text":"%pyspark\n\n# Import neccessary packages\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pyspark.sql.types import IntegerType, StringType\n\n### Pandas UDF to get the length of the name\nslen = pandas_udf(lambda s: s.str.len(), IntegerType())  \n\n### Pandas UDF\n:pandas_udf(StringType())  \ndef to_upper(s):\n    return s.str.upper()\n\n:pandas_udf(\"integer\", PandasUDFType.SCALAR)  \n\ndef add_one(x):\n    return x + 1\n\ndf = spark.createDataFrame([(1, \"John Doe\", 21)],\n                           (\"id\", \"name\", \"age\"))  \ndf.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\n    .show()  ","user":"anonymous","dateUpdated":"2019-11-13T16:52:09-0600","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1573685529583_588553975","id":"20180323-155722_423508495","dateCreated":"2019-11-13T16:52:09-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"text":"%md\n### GROUPED_MAP\n\n* A grouped map UDF defines transformation: A pandas.DataFrame -> A pandas.DataFrame The returnType should be a StructType describing the schema of the returned pandas.DataFrame. The length of the returned pandas.DataFrame can be arbitrary.\n\n* Grouped map UDFs are used with pyspark.sql.GroupedData.apply().\n","user":"anonymous","dateUpdated":"2019-11-13T18:26:34-0600","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"fontSize":9,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>GROUPED_MAP</h3>\n<ul>\n  <li>\n  <p>A grouped map UDF defines transformation: A pandas.DataFrame -&gt; A pandas.DataFrame The returnType should be a StructType describing the schema of the returned pandas.DataFrame. The length of the returned pandas.DataFrame can be arbitrary.</p></li>\n  <li>\n  <p>Grouped map UDFs are used with pyspark.sql.GroupedData.apply().</p></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529583_1935851646","id":"20180323-161129_1349244524","dateCreated":"2019-11-13T16:52:09-0600","dateStarted":"2019-11-13T16:53:03-0600","dateFinished":"2019-11-13T16:53:03-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"text":"%pyspark\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\ndf = spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n    (\"id\", \"v\"))  \n:pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  \ndef normalize(pdf):\n    v = pdf.v\n    return pdf.assign(v=(v - v.mean()) / v.std())\ndf.groupby(\"id\").apply(normalize).show()","user":"anonymous","dateUpdated":"2019-11-13T16:52:09-0600","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1573685529584_758269427","id":"20180323-161128_2061483320","dateCreated":"2019-11-13T16:52:09-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"text":"%md\nNote The user-defined functions are considered deterministic by default. \nDue to optimization, duplicate invocations may be eliminated or the function may even be invoked more times than it is present in the query. \nIf your function is not deterministic, call asNondeterministic on the user defined function. E.g.:\n\n","user":"anonymous","dateUpdated":"2019-11-13T16:54:17-0600","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Note The user-defined functions are considered deterministic by default.<br/>Due to optimization, duplicate invocations may be eliminated or the function may even be invoked more times than it is present in the query.<br/>If your function is not deterministic, call asNondeterministic on the user defined function. E.g.:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529585_-175845600","id":"20180323-161127_814627164","dateCreated":"2019-11-13T16:52:09-0600","dateStarted":"2019-11-13T16:54:17-0600","dateFinished":"2019-11-13T16:54:17-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%pyspark\n:pandas_udf('double', PandasUDFType.SCALAR)  \ndef random(v):\n    import numpy as np\n    import pandas as pd\n    return pd.Series(np.random.randn(len(v))\nrandom = random.asNondeterministic()  ","user":"anonymous","dateUpdated":"2019-11-13T16:52:09-0600","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1573685529586_-379928471","id":"20180323-161126_923827632","dateCreated":"2019-11-13T16:52:09-0600","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"text":"%md\n### GROUPED_AGGREGATE_UDF's\n\n* Grouped aggregate pandas UDFs are similar to Spark aggregate functions. we use grouped aggregate pandas UDFs with groupBy().agg() and pyspark.sql.Window.\n* A grouped aggregate UDF defines an aggregation from one or more pandas.Series to a scalar value, where each pandas.Series represents a column within the group or window.\n* This type of UDF does not support partial aggregation and all data for a group or window is loaded into memory. Also, only unbounded window is supported with grouped aggregate pandas UDFs.\n","user":"anonymous","dateUpdated":"2019-11-13T18:29:36-0600","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"fontSize":9,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>GROUPED_AGGREGATE_UDF&rsquo;s</h3>\n<ul>\n  <li>Grouped aggregate pandas UDFs are similar to Spark aggregate functions. we use grouped aggregate pandas UDFs with groupBy().agg() and pyspark.sql.Window.</li>\n  <li>A grouped aggregate UDF defines an aggregation from one or more pandas.Series to a scalar value, where each pandas.Series represents a column within the group or window.</li>\n  <li>This type of UDF does not support partial aggregation and all data for a group or window is loaded into memory. Also, only unbounded window is supported with grouped aggregate pandas UDFs.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1573685529587_-1199827131","id":"20180323-164125_1417210635","dateCreated":"2019-11-13T16:52:09-0600","dateStarted":"2019-11-13T18:29:36-0600","dateFinished":"2019-11-13T18:29:36-0600","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%pyspark\n\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pyspark.sql import Window\n\ndf = spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n    (\"id\", \"v\"))\n\n@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\ndef mean_udf(v):\n    return v.mean()\n\ndf.groupby(\"id\").agg(mean_udf(df['v'])).show()\n\n\nw = Window \\\n    .partitionBy('id') \\\n    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\ndf.withColumn('mean_v', mean_udf(df['v']).over(w)).show()\n\n\n","user":"anonymous","dateUpdated":"2019-11-13T18:30:16-0600","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1573691376406_1170176675","id":"20191113-182936_835742828","dateCreated":"2019-11-13T18:29:36-0600","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:65"}],"name":"Day2_Session1/Pyspark_PandasUDF","id":"2ESS76M9T","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}